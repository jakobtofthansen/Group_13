{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Data Science Exam - 2019\n",
    "\n",
    "## Group 13\n",
    "\n",
    "Nynne Bech Nielsen (btq674) \\\n",
    "Marcus Bjarup Thøgersen (vhp312) \\\n",
    "Kaiyue Xu (vsp923) \\\n",
    "Jakob Lauge Toft Hansen (qkr676)\n",
    "\n",
    "### Python code for all plots and scraping\n",
    "\n",
    "This Notebook contains the code for the data used in the final exam for group 13, in the course Social Data Science 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for the Log, made by Snorre.\n",
    "\n",
    "import requests,os,time\n",
    "def ratelimit():\n",
    "    \"A function that handles the rate of your calls.\"\n",
    "    time.sleep(1) # sleep one second.\n",
    "\n",
    "class Connector():\n",
    "  def __init__(self,logfile,overwrite_log=False,connector_type='requests',session=False,path2selenium='',n_tries = 5,timeout=30):\n",
    "    \"\"\"This Class implements a method for reliable connection to the internet and monitoring. \n",
    "    It handles simple errors due to connection problems, and logs a range of information for basic quality assessments\n",
    "    \n",
    "    Keyword arguments:\n",
    "    logfile -- path to the logfile\n",
    "    overwrite_log -- bool, defining if logfile should be cleared (rarely the case). \n",
    "    connector_type -- use the 'requests' module or the 'selenium'. Will have different since the selenium webdriver does not have a similar response object when using the get method, and monitoring the behavior cannot be automated in the same way.\n",
    "    session -- requests.session object. For defining custom headers and proxies.\n",
    "    path2selenium -- str, sets the path to the geckodriver needed when using selenium.\n",
    "    n_tries -- int, defines the number of retries the *get* method will try to avoid random connection errors.\n",
    "    timeout -- int, seconds the get request will wait for the server to respond, again to avoid connection errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialization function defining parameters. \n",
    "    self.n_tries = n_tries # For avoiding triviel error e.g. connection errors, this defines how many times it will retry.\n",
    "    self.timeout = timeout # Defining the maximum time to wait for a server to response.\n",
    "    ## not implemented here, if you use selenium.\n",
    "    if connector_type=='selenium':\n",
    "      assert path2selenium!='', \"You need to specify the path to you geckodriver if you want to use Selenium\"\n",
    "      from selenium import webdriver \n",
    "      ## HIN download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "      assert os.path.isfile(path2selenium),'You need to insert a valid path2selenium the path to your geckodriver. You can download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases'\n",
    "      self.browser = webdriver.Firefox(executable_path=path2selenium) # start the browser with a path to the geckodriver.\n",
    "\n",
    "    self.connector_type = connector_type # set the connector_type\n",
    "    \n",
    "    if session: # set the custom session\n",
    "      self.session = session\n",
    "    else:\n",
    "      self.session = requests.session()\n",
    "    self.logfilename = logfile # set the logfile path\n",
    "    ## define header for the logfile\n",
    "    header = ['id','project','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "    if os.path.isfile(logfile):        \n",
    "      if overwrite_log==True:\n",
    "        self.log = open(logfile,'w')\n",
    "        self.log.write(';'.join(header))\n",
    "      else:\n",
    "        self.log = open(logfile,'a')\n",
    "    else:\n",
    "      self.log = open(logfile,'w')\n",
    "      self.log.write(';'.join(header))\n",
    "    ## load log \n",
    "    with open(logfile,'r') as f: # open file\n",
    "        \n",
    "      l = f.read().split('\\n') # read and split file by newlines.\n",
    "      ## set id\n",
    "      if len(l)<=1:\n",
    "        self.id = 0\n",
    "      else:\n",
    "        self.id = int(l[-1][0])+1\n",
    "            \n",
    "  def get(self,url,project_name):\n",
    "    \"\"\"Method for connector reliably to the internet, with multiple tries and simple error handling, as well as default logging function.\n",
    "    Input url and the project name for the log (i.e. is it part of mapping the domain, or is it the part of the final stage in the data collection).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- str, url\n",
    "    project_name -- str, Name used for analyzing the log. Use case could be the 'Mapping of domain','Meta_data_collection','main data collection'. \n",
    "    \"\"\"\n",
    "     \n",
    "    project_name = project_name.replace(';','-') # make sure the default csv seperator is not in the project_name.\n",
    "    if self.connector_type=='requests': # Determine connector method.\n",
    "      for _ in range(self.n_tries): # for loop defining number of retries with the requests method.\n",
    "        ratelimit()\n",
    "        t = time.time()\n",
    "        try: # error handling \n",
    "          response = self.session.get(url,timeout = self.timeout) # make get call\n",
    "\n",
    "          err = '' # define python error variable as empty assumming success.\n",
    "          success = True # define success variable\n",
    "          redirect_url = response.url # log current url, after potential redirects \n",
    "          dt = t - time.time() # define delta-time waiting for the server and downloading content.\n",
    "          size = len(response.text) # define variable for size of html content of the response.\n",
    "          response_code = response.status_code # log status code.\n",
    "          ## log...\n",
    "          call_id = self.id # get current unique identifier for the call\n",
    "          self.id+=1 # increment call id\n",
    "          #['id','project_name','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row to be written in the log.\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write log.\n",
    "          return response,call_id # return response and unique identifier.\n",
    "\n",
    "        except Exception as e: # define error condition\n",
    "          err = str(e) # python error\n",
    "          response_code = '' # blank response code \n",
    "          success = False # call success = False\n",
    "          size = 0 # content is empty.\n",
    "          redirect_url = '' # redirect url empty \n",
    "          dt = t - time.time() # define delta t\n",
    "\n",
    "          ## log...\n",
    "          call_id = self.id # define unique identifier\n",
    "          self.id+=1 # increment call_id\n",
    "\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write row to log.\n",
    "    else:\n",
    "      t = time.time()\n",
    "      ratelimit()\n",
    "      self.browser.get(url) # use selenium get method\n",
    "      ## log\n",
    "      call_id = self.id # define unique identifier for the call. \n",
    "      self.id+=1 # increment the call_id\n",
    "      err = '' # blank error message\n",
    "      success = '' # success blank\n",
    "      redirect_url = self.browser.current_url # redirect url.\n",
    "      dt = t - time.time() # get time for get method ... NOTE: not necessarily the complete load time.\n",
    "      size = len(self.browser.page_source) # get size of content ... NOTE: not necessarily correct, since selenium works in the background, and could still be loading.\n",
    "      response_code = '' # empty response code.\n",
    "      row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row \n",
    "      self.log.write('\\n'+';'.join(map(str,row))) # write row to log file.\n",
    "    # Using selenium it will not return a response object, instead you should call the browser object of the connector.\n",
    "    ## connector.browser.page_source will give you the html.\n",
    "      return call_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the final code for our exam!\n",
    "\n",
    "### Here we scrape the tripadvisor page for all restaurants in Copenhagen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages \n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import requests,os,re\n",
    "from time import sleep\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = Connector('logfile_sds_trip_all_res.csv')\n",
    "\n",
    "# Header\n",
    "session = requests.session()\n",
    "session.headers['emails'] = \"vhp312@alumni.ku.dk\"\n",
    "session.headers['names'] = \"Nynne Bech Nielsen (btq674) Marcus Bjarup Thøgersen (vhp312) Kaiyue Xu (vsp923) Jakob Lauge Toft Hansen (qkr676)\"\n",
    "session.headers['description'] = \"Til brug for eksamen i Social Data Science, \\\n",
    "                                  KU (https://kurser.ku.dk/course/aØkk08216u/2018-2019)\"\n",
    "\n",
    "# Getting all the links from the tripadvisor page. Stores it in a list called 'links'\n",
    "links=[]\n",
    "for nummer in list(range(0, 30*78, 30)):\n",
    "    sleep(0.5)\n",
    "    url_nr = 'https://www.tripadvisor.dk/Restaurants-g189541-oa{}-Copenhagen_Zealand.html'.format(nummer)\n",
    "    response, call_id = connector.get(url_nr,'scraping restaurants')\n",
    "    links.append(url_nr)\n",
    "#print(links)\n",
    "\n",
    "pd.read_csv('logfile_sds_trip_all_res.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function takes an url as input. \n",
    "def get_info(link):\n",
    "    sleep(0.5)\n",
    "    print(\"Slept 0.5, getting \", link)\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        N, B, R= [], [], []\n",
    "        for items in soup.find_all(class_=\"shortSellDetails\"):\n",
    "            try:\n",
    "                name = items.find(class_=\"property_title\").get_text(strip=True) ## Creating a function that searches for the restaurant name\n",
    "            except:\n",
    "                name = ''\n",
    "            try:\n",
    "                bubble = items.find(class_=\"ui_bubble_rating\").get(\"alt\") #rating\n",
    "            except:\n",
    "                bubble = ''\n",
    "            try:\n",
    "                review = items.find(class_=\"reviewCount\").get_text(strip=True)#and how many reviews\n",
    "            except:\n",
    "                review = ''\n",
    "            N.append(name)\n",
    "            B.append(bubble)\n",
    "            R.append(review)\n",
    "        return N,B,R\n",
    "\n",
    "# Loops over all the links in the list of links and appends all the data to a dataframe.   \n",
    "data = []\n",
    "for link in links:\n",
    "    n,b,r = get_info(link)\n",
    "    data.append(pd.DataFrame({'name': n, 'bubble': b, 'review':r}))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip = pd.concat(data)\n",
    "trip_csv = trip.to_csv(r'/Users/marcusbjarupthogersen/Documents/Group_13/trip_csv.csv')\n",
    "trip_sorted = pd.read_csv('/Users/marcusbjarupthogersen/Documents/Group_13/trip_csv.csv')\n",
    "trip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting rows with NaN and duplicates \n",
    "trip_sorted = trip_sorted.dropna()\n",
    "trip_sorted.drop_duplicates(subset =\"name\", inplace = True)\n",
    "\n",
    "trip_csv2 = trip_sorted.to_csv(r'/Users/marcusbjarupthogersen/Documents/Group_13/trip_csv2.csv')\n",
    "\n",
    "# Deletes the thousand separator in order to make the d.object into an integer. \n",
    "trip_sorted = trip_sorted.astype(str).apply(lambda x: x.str.replace('.',''))\n",
    "\n",
    "# Replaces the comma with a dot in order to make the d.object into a float\n",
    "trip_sorted = trip_sorted.astype(str).apply(lambda x: x.str.replace(',','.'))\n",
    "\n",
    "# Extracts the number from reviews count and stores it in another column as an integer\n",
    "trip_sorted['reviews int'] = trip_sorted['review'].str.extract('(\\d+)').astype(int)\n",
    "#trip_sorted['bubbles int'] = trip_sorted['bubble'].str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Creates a new column with the rating by deleting the sentence \"ud af 5 bobler\"\n",
    "trip_sorted['bubbles float'] = trip_sorted['bubble'].replace(to_replace = r' ud af 5 bobler', value = '', regex=True)\n",
    "\n",
    "# Converts the rating-column into a float in order to sort on this later.\n",
    "trip_sorted['bubbles float'] = trip_sorted['bubbles float'].astype(str).astype(float)\n",
    "\n",
    "# Sorts the data on reviews count\n",
    "trip_sorted.sort_values(by = ['reviews int'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we scrape all the reviews for the three chosen restaurants: Mother, Kødbyens Fiskebar and restaurant tight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages \n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import requests,os,re\n",
    "from time import sleep\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we scrape mother\n",
    "\n",
    "# Header\n",
    "session = requests.session()\n",
    "session.headers['emails'] = \"vhp312@alumni.ku.dk\"\n",
    "session.headers['names'] = \"Nynne Bech Nielsen (btq674) Marcus Bjarup Thøgersen (vhp312) Kaiyue Xu (vsp923) Jakob Lauge Toft Hansen (qkr676)\"\n",
    "session.headers['description'] = \"Til brug for eksamen i Social Data Science, KU (https://kurser.ku.dk/course/aØkk08216u/2018-2019)\"\n",
    "\n",
    "connector = Connector('logfile_sds_trip_mother.csv')\n",
    "\n",
    "# Getting all the links from the tripadvisor(mother) page. There are 192 pages of reviews with 10 reviews on each page\n",
    "# Stores it in a list called 'links(Marcus) or listen(Jakob)'\n",
    "links_mother=[]\n",
    "for nummer in list(range(0, 10*192, 10)): #192 is the number of pages on tripadvisor\n",
    "    sleep(0.5)\n",
    "    url_nr = 'https://www.tripadvisor.com/Restaurant_Review-g189541-d1898372-Reviews-or{}-Mother-Copenhagen_Zealand.html'.format(nummer)\n",
    "    response, call_id = connector.get(url_nr,'scraping restaurants') # make the log-file for data validation\n",
    "    links_mother.append(url_nr)\n",
    "\n",
    "pd.read_csv('logfile_sds_trip_mother.csv',sep=';') # examine the logfile results. It seemes that it iterated over the same link to many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function that searches for the review, rating (bubble) and location. \n",
    "### The function takes an url as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connector = Connector('logfile_sds_trip_mother.csv')\n",
    "\n",
    "def get_info(link):\n",
    "    sleep(0.5)\n",
    "    print(\"Slept 0.5, getting \", link)\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        N, L, M = [], [], []\n",
    "        for items in soup.find_all(class_=\"reviewSelector\"):\n",
    "            try:\n",
    "                name = items.find(class_=\"partial_entry\").get_text(strip=True)\n",
    "            except:\n",
    "                name = ''\n",
    "            try:\n",
    "                location = items.find(class_=\"userLoc\").get_text(strip=True)\n",
    "            except:\n",
    "                location = ''\n",
    "            try:\n",
    "                bubble = items.find(class_=\"ui_bubble_rating\")\n",
    "            except:\n",
    "                bubble = ''\n",
    "            N.append(name)\n",
    "            L.append(location)\n",
    "            M.append(bubble)\n",
    "\n",
    "        return N, L, M\n",
    "\n",
    "# Loops over all the links in the list of links and appends all the data to a dataframe.   \n",
    "data_mother = []\n",
    "for link in links_mother:\n",
    "    n, l, m = get_info(link)\n",
    "    response, call_id = connector.get(link,'scraping mother')\n",
    "    data_mother.append(pd.DataFrame({'name': n, 'location': l, 'bubble':m}))\n",
    "\n",
    "pd.read_csv('logfile_sds_trip_mother.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trip_mother = pd.concat(data_mother)\n",
    "\n",
    "trip_mother['bubble'][10:]\n",
    "\n",
    "# start stop and step variables \n",
    "start, stop, step = 37, -9, 1\n",
    "\n",
    "# slicing to integer\n",
    "trip_mother[\"rating\"]= trip_mother[\"bubble\"].astype(str).str.slice(start, stop, step).astype(int) / 10 \n",
    "  \n",
    "trip_mother = trip_mother.reset_index(drop=True)\n",
    "trip_mother "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_mother = trip_mother.to_csv(r'/Users/marcusbjarupthogersen/Desktop/SDS_Eksamen/dataframe_mother2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kødbyens Fiskebar\n",
    "# Header\n",
    "session = requests.session()\n",
    "session.headers['emails'] = \"vhp312@alumni.ku.dk\"\n",
    "session.headers['names'] = \"Nynne Bech Nielsen (btq674) Marcus Bjarup Thøgersen (vhp312) Kaiyue Xu (vsp923) Jakob Lauge Toft Hansen (qkr676)\"\n",
    "session.headers['description'] = \"Til brug for eksamen i Social Data Science, KU (https://kurser.ku.dk/course/aØkk08216u/2018-2019)\"\n",
    "\n",
    "connector = Connector('logfile_sds_trip_fiskebar.csv')\n",
    "\n",
    "# Getting all the links from the tripadvisor(Kødbyens Fiskebar) page. There are 242 pages of reviews with 10 reviews on \n",
    "# each page. Stores it in a list called 'links_fiskebar'\n",
    "links_fiskebar=[]\n",
    "for nummer in list(range(0, 10*242, 10)):\n",
    "    sleep(0.5)\n",
    "    url_nr = 'https://www.tripadvisor.com/Restaurant_Review-g189541-d2085491-Reviews-or{}-Kodbyens_Fiskebar-Copenhagen_Zealand.html'.format(nummer)\n",
    "    response, call_id = connector.get(url_nr,'scraping kødbyens fiskebar')\n",
    "    links_fiskebar.append(url_nr)\n",
    "    \n",
    "pd.read_csv('logfile_sds_trip_fiskebar.csv',sep=';')\n",
    "print(links_fiskebar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_fiskebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = Connector('logfile_sds_trip_fiskebar.csv')\n",
    "\n",
    "\n",
    "def get_info(link):\n",
    "    sleep(0.5)\n",
    "    print(\"Slept 0.5, getting \", link)\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        N, L, M = [], [], []\n",
    "        for items in soup.find_all(class_=\"reviewSelector\"):\n",
    "            try:\n",
    "                name = items.find(class_=\"partial_entry\").get_text(strip=True)\n",
    "            except:\n",
    "                name = ''\n",
    "            try:\n",
    "                location = items.find(class_=\"userLoc\").get_text(strip=True)\n",
    "            except:\n",
    "                location = ''\n",
    "            try:\n",
    "                bubble = items.find(class_=\"ui_bubble_rating\")\n",
    "            except:\n",
    "                bubble = ''\n",
    "            N.append(name)\n",
    "            L.append(location)\n",
    "            M.append(bubble)\n",
    "\n",
    "        return N, L, M\n",
    "\n",
    "# Loops over all the links in the list of links and appends all the data to a dataframe. \n",
    "data_fiskebar = []\n",
    "for link in links_fiskebar:\n",
    "    n, l, m = get_info(link)\n",
    "    response, call_id = connector.get(link,'scraping kødbyens fiskebar reviews')\n",
    "    data_fiskebar.append(pd.DataFrame({'name': n, 'location': l, 'bubble':m}))\n",
    "\n",
    "pd.read_csv('logfile_sds_trip_fiskebar.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kødbyens fiskebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trip_fiskebar = pd.concat(data_fiskebar)\n",
    "\n",
    "trip_fiskebar['bubble'][10:]\n",
    "\n",
    "# start stop and step variables \n",
    "start, stop, step = 37, -9, 1\n",
    "\n",
    "# slicing to integer\n",
    "trip_fiskebar[\"rating\"]= trip_fiskebar[\"bubble\"].astype(str).str.slice(start, stop, step).astype(int) / 10 \n",
    "  \n",
    "trip_fiskebar = trip_fiskebar.reset_index(drop=True)\n",
    "trip_fiskebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaurant Tight\n",
    "# Header\n",
    "session = requests.session()\n",
    "session.headers['emails'] = \"vhp312@alumni.ku.dk\"\n",
    "session.headers['names'] = \"Nynne Bech Nielsen (btq674) Marcus Bjarup Thøgersen (vhp312) Kaiyue Xu (vsp923) Jakob Lauge Toft Hansen (qkr676)\"\n",
    "session.headers['description'] = \"Til brug for eksamen i Social Data Science, KU (https://kurser.ku.dk/course/aØkk08216u/2018-2019)\"\n",
    "\n",
    "connector = Connector('logfile_sds_trip_tight.csv')\n",
    "\n",
    "# Getting all the links from the tripadvisor(Restaurant Tight) page. There are 287 pages of reviews with 10 reviews on \n",
    "# each page. Stores it in a list called 'links_fiskebar'\n",
    "links_tight=[]\n",
    "for nummer in list(range(0, 10*287, 10)):\n",
    "    sleep(0.5)\n",
    "    url_nr = 'https://www.tripadvisor.dk/Restaurant_Review-g189541-d1528309-Reviews-or{}-Restaurant_Tight-Copenhagen_Zealand.html'.format(nummer)\n",
    "    \n",
    "    response, call_id = connector.get(url_nr,'scraping Restaurant Tight')\n",
    "\n",
    "pd.read_csv('logfile_sds_trip_tight.csv',sep=';')\n",
    "#print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = Connector('logfile_sds_trip_tight.csv')\n",
    "\n",
    "def get_info(link):\n",
    "    sleep(0.5)\n",
    "    print(\"Slept 0.5, getting \", link)\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        N, L, M = [], [], []\n",
    "        for items in soup.find_all(class_=\"reviewSelector\"):\n",
    "            try:\n",
    "                name = items.find(class_=\"partial_entry\").get_text(strip=True)\n",
    "            except:\n",
    "                name = ''\n",
    "            try:\n",
    "                location = items.find(class_=\"userLoc\").get_text(strip=True)\n",
    "            except:\n",
    "                location = ''\n",
    "            try:\n",
    "                bubble = items.find(class_=\"ui_bubble_rating\")\n",
    "            except:\n",
    "                bubble = ''\n",
    "            N.append(name)\n",
    "            L.append(location)\n",
    "            M.append(bubble)\n",
    "\n",
    "        return N, L, M\n",
    "\n",
    "\n",
    "# Loops over all the links in the list of links and appends all the data to a dataframe.   \n",
    "data_tight = []\n",
    "for link in links_tight:\n",
    "    n, l, m = get_info(link)\n",
    "    response, call_id = connector.get(link,'Restaurant Tight reviews')\n",
    "    data_tight.append(pd.DataFrame({'name': n, 'location': l, 'bubble':m}))\n",
    "\n",
    "pd.read_csv('logfile_sds_trip_tight.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_tight = pd.concat(data_tight)\n",
    "\n",
    "trip_tight['bubble'][10:]\n",
    "\n",
    "# start stop and step variables \n",
    "start, stop, step = 37, -9, 1 # isolating the rating\n",
    "\n",
    "# slicing to integer\n",
    "trip_tight[\"rating\"]= trip_tight[\"bubble\"].astype(str).str.slice(start, stop, step).astype(int) / 10 \n",
    "  \n",
    "trip_tight = trip_fiskebar.reset_index(drop=True)\n",
    "trip_tight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mother = pd.read_csv('logfile_sds_trip_mother.csv', sep=';')\n",
    "log_mother.describe().to_csv('description_log.csv')\n",
    "log_mother.plot(y='delta_t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "### Now we do the sentiment s analysis. We included the code for the different  sintiments analysis presented in the course. The final choice were the vader analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import numpy as np\n",
    "seaborn as sns\n",
    "pandas as pd\n",
    "\n",
    "## For text classification:\n",
    "import nltk, nltk.sentiment, sklearn\n",
    "%matplotlib inline\n",
    "\n",
    "### Download data as pandas dataframe\n",
    "import requests\n",
    "path2data = 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv'\n",
    "df = pd.read_csv(path2data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative sentiment lexicons \n",
    "negative = set(requests.get('http://ptrckprry.com/course/ssd/data/negative-words.txt').text.split(';\\n')[-1].split('\\n'))\n",
    "positive = set(requests.get('http://ptrckprry.com/course/ssd/data/positive-words.txt').text.split(';\\n')[-1].split('\\n'))\n",
    "print(len(negative),len(positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the respective csv.files\n",
    "\n",
    "df1 = pd.read_csv (r'C:\\Users\\jtoft\\Downloads\\dataframe_tight_v2.csv')\n",
    "df2 = pd.read_csv (r'C:\\Users\\jtoft\\Downloads\\dataframe_fiskebar.csv')\n",
    "df3 = pd.read_csv (r'C:\\Users\\jtoft\\Downloads\\dataframe_mother.csv')\n",
    "\n",
    "frames = [df1, df2, df3]\n",
    "\n",
    "#df = pd.read_csv (r'C:\\Users\\jtoft\\Downloads\\dataframe_all3.csv')\n",
    "\n",
    "df = pd.concat(frames)\n",
    "df.reset_index(drop=True)\n",
    "df = df.loc[~df.index.duplicated(keep='first')]\n",
    "#df = df.to_csv(r'C:\\Users\\jtoft\\Downloads\\dataframe_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] = df['location'].str.rsplit(',').str[-1] # isolating the country as a variable in column \n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "# define function\n",
    "def preprocessing(string):\n",
    "    return tokenizer.tokenize(string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.name.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define count function using a list comprehension.\n",
    "def count_dictionary(tokenized_doc,dictionary):\n",
    "    return len([word for word in tokenized_doc if word in dictionary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive_liu'] = documents.apply(count_dictionary,dictionary=positive)\n",
    "df['negative_liu'] = documents.apply(count_dictionary,dictionary=negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk.sentiment\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "import nltk.sentiment\n",
    "# initialize the vader function\n",
    "vader = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "# apply the function and convert to dataframe\n",
    "vader_df = pd.DataFrame(list(df['name'].apply(vader.polarity_scores)))\n",
    "# rename columns adding the 'vader_' prefix using a list comprehension\n",
    "vader_df.columns = ['vader_'+col for col in vader_df.columns]\n",
    "# merge with original dataframe\n",
    "df = pd.concat([df,vader_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn #importing one of the methods for sentiment analysis\n",
    "afinn = Afinn()\n",
    "df['afinn'] = df.name.apply(afinn.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define columns\n",
    "sentiment_columns = ['afinn','positive_liu','negative_liu']+[col for col in df.columns if 'vader_' in col]\n",
    "hue = 'rating'\n",
    "sns.pairplot(df.sample(2000)[sentiment_columns+[hue]],hue=hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check positive LIU classification\n",
    "positive_liu_reviews = df[((df.positive_liu-df.negative_liu)>0)]\n",
    "\n",
    "# #check words with LUI classified as positive:\n",
    "for idx in df.loc[df[((df.positive_liu-df.negative_liu)>0)].vader_compound.sort_values().index].index:\n",
    "    print(idx,set(documents[idx])&positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = df.drop_duplicates(subset=['name'])\n",
    "df2.groupby('tourism').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'].astype(str) # change type from 'object' to string\n",
    "\n",
    "df['tourism'] = np.where(dft['country'] == ' Denmark', 'Local', 'Tourist')#when splitting, there were a space in country column\n",
    "\n",
    "df.groupby('tourism').count() # create categories local and tourist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"rating\", y=\"vader_compound\", hue=\"tourism\", kind=\"swarm\", data=df) #make some quick plots to analyze\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"rating\", y=\"vader_compound\", hue=\"tourism\", kind=\"box\", data=df);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from matplotlib.patches import Polygon\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.style.use(['ggplot']) # optional: for ggplot-like style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the final plot, a boxplot to illustrate rating and reviews between categories.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Draw Plot\n",
    "plt.figure(figsize=(13,10), dpi= 80)\n",
    "sns.boxplot(x='rating', y='vader_compound', data=df, hue='tourism')\n",
    "#sns.stripplot(x='rating', y='vader_compound', data=df, color='black', size=2, jitter=1)\n",
    "\n",
    "for i in range(len(df['tourism'].unique())-1):\n",
    "    plt.vlines(i+.1, 1, 1, linestyles='solid', colors='gray', alpha=0.4)\n",
    "\n",
    "# Decoration\n",
    "plt.title('Reviews - local vs. tourist', fontsize=22)\n",
    "plt.legend(title='Category', fontsize=12)\n",
    "fig1 = plt.gcf()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "fig1.savefig('boxplot_category.png', dpi=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statstik der beskriver data\n",
    "\n",
    "stat = df.iloc[:,9:16:5] \n",
    "stat2 = df.groupby('tourism').describe()\n",
    "\n",
    "stat2.to_csv(r'C:\\Users\\jtoft\\Downloads\\statestik.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following sections make the graphs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# create a dataFrame object with title, score and release_year\n",
    "df = pd.read_csv(\"local_freq.csv\")[[\"name\", \"location\", \"bubble\", \"dollars\", \"rating\"]]\n",
    "df['country'] = df['location'].str.rsplit(',').str[-1] \n",
    "df.reset_index(drop=True)\n",
    "df['country'].astype(str) # change type from 'object' to string\n",
    "\n",
    "# lande = {' Denmark', ' Sweden', ' Norway', ' Finland', ' Iceland'}\n",
    "\n",
    "\n",
    "df['tourism'] = np.where(df['country'] == ' Denmark', 'Local', 'Tourist')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "out = list()\n",
    "for sentence in df['name'].apply(lambda x: x.split()):\n",
    "    for word in sentence:\n",
    "        out.append(word)\n",
    "\n",
    "word_freq  = dict(Counter(out))\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and keep x highest values\n",
    "# sorted(word_freq, key= word_freq.get, reverse=True)\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "wordlist = dict(OrderedDict(sorted(word_freq.items(), key=itemgetter(1), reverse=True)[18:50]))\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "plt.subplots(figsize=(18,10))\n",
    "plt.barh(range(len(wordlist)),list(wordlist.values()), align='center')\n",
    "plt.yticks(range(len(wordlist)), list(wordlist.keys()))\n",
    "plt.ylabel('words')\n",
    "#define values\n",
    "values = wordlist.values()\n",
    "plt.xlabel('frequency')\n",
    "plt.title('Frequent words of locals')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tourist_freq.csv\")[[\"name\", \"location\", \"bubble\", \"dollars\", \"rating\"]]\n",
    "df['country'] = df['location'].str.rsplit(',').str[-1] \n",
    "df.reset_index(drop=True)\n",
    "df['country'].astype(str) # change type from 'object' to string\n",
    "\n",
    "# lande = {' Denmark', ' Sweden', ' Norway', ' Finland', ' Iceland'}\n",
    "\n",
    "\n",
    "df['tourism'] = np.where(df['country'] == ' Denmark', 'Local', 'Tourist')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "out = list()\n",
    "for sentence in df['name'].apply(lambda x: x.split()):\n",
    "    for word in sentence:\n",
    "        out.append(word)\n",
    "\n",
    "word_freq  = dict(Counter(out))\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "wordlist = dict(OrderedDict(sorted(word_freq.items(), key=itemgetter(1), reverse=True)[18:50]))\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "plt.subplots(figsize=(18,10))\n",
    "plt.barh(range(len(wordlist)),list(wordlist.values()), align='center')\n",
    "plt.yticks(range(len(wordlist)), list(wordlist.keys()))\n",
    "plt.ylabel('words')\n",
    "#define values\n",
    "values = wordlist.values()\n",
    "plt.xlabel('frequency')\n",
    "plt.title('Frequent words of tourists')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from subprocess import check_output\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "mpl.rcParams['figure.figsize']=(6.0,4.0)    #(6.0,4.0)\n",
    "mpl.rcParams['font.size']=10               #10 \n",
    "mpl.rcParams['savefig.dpi']= 72        #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stopwords = [\"restaurant\", \"Pizza\", \"Tight\",\"els\",\"back\",\"wi\",\"pi\",\"ve\",\"ano\",\"delish\"] + list(STOPWORDS)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stopwords,\n",
    "                          max_words=100,\n",
    "                          max_font_size=40, \n",
    "                          random_state=42\n",
    "                         ).generate(str(data['name']))\n",
    "\n",
    "print(wordcloud)\n",
    "\n",
    "fig = plt.figure(1, figsize = (12,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "fig.savefig(\"word1.png\", dpi=900)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deleting rows with NaN and duplicates \n",
    "# trip_sorted = trip_sorted.dropna()\n",
    "# trip_sorted.drop_duplicates(subset =\"name\"\n",
    "#                             , inplace = True) #\n",
    "\n",
    "trip_sorted = pd.read_csv(r'/Users/KaiyueXu/Desktop/test.csv')\n",
    "\n",
    "# Deletes the thousand separator in order to make the d.object into an integer. \n",
    "trip_sorted = trip_sorted.astype(str).apply(lambda x: x.str.replace('.',''))\n",
    "\n",
    "# Replaces the comma with a dot in order to make the d.object into a float\n",
    "trip_sorted = trip_sorted.astype(str).apply(lambda x: x.str.replace(',','.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the number from reviews count and stores it in another column as an integer\n",
    "trip_sorted['reviews int'] = trip_sorted['review'].str.extract('(\\d+)').astype(int)\n",
    "#trip_sorted['bubbles int'] = trip_sorted['bubble'].str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Creates a new column with the rating by deleting the sentence \"ud af 5 bobler\"\n",
    "trip_sorted['bubbles int'] = trip_sorted['bubble'].replace(to_replace = r' ud af 5 bobler', value = '', regex=True)\n",
    "\n",
    "# Converts the rating-column into a float in order to sort on this later.\n",
    "trip_sorted['bubbles int'] = trip_sorted['bubbles int'].astype(str).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts the data on reviews count\n",
    "trip_rank = trip_sorted.sort_values(by = ['reviews int'], ascending = False).drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1).head(5)\n",
    "trip_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Reviews = [1919,2021,2135,2417,2826]\n",
    "\n",
    "x = ['Mother','Höst','The Olive Kitchen &Bar','Kodbyens Fiskebar','Restaurant Tight']\n",
    "\n",
    "plt.barh(range(5), Reviews, 0.4,color='b', alpha = 0.8)\n",
    "\n",
    "plt.ylabel('Restaurant')\n",
    "\n",
    "plt.xlabel('Reviews')\n",
    "\n",
    "plt.title('Top 5 most reviews restaurants')\n",
    "\n",
    "plt.yticks(range(5),['Mother','Höst','The Olive Kitchen &Bar','Kodbyens Fiskebar','Restaurant Tight'])\n",
    "\n",
    "plt.xlim([1500,3000])\n",
    "\n",
    "\n",
    "for x,y in enumerate(Reviews):\n",
    "    plt.text(y+0.2,x,'%s' %y,va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the code that gets the Danish reviews in the expanded version. The data is not alalyzed in the research paper  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOLUTION\n",
    "\n",
    "import requests,os,time\n",
    "def ratelimit():\n",
    "    \"A function that handles the rate of your calls.\"\n",
    "    time.sleep(1) # sleep one second.\n",
    "\n",
    "class Connector():\n",
    "  def __init__(self,logfile,overwrite_log=False,connector_type='requests',session=False,path2selenium='',n_tries = 5,timeout=30):\n",
    "    \"\"\"This Class implements a method for reliable connection to the internet and monitoring. \n",
    "    It handles simple errors due to connection problems, and logs a range of information for basic quality assessments\n",
    "    \n",
    "    Keyword arguments:\n",
    "    logfile -- path to the logfile\n",
    "    overwrite_log -- bool, defining if logfile should be cleared (rarely the case). \n",
    "    connector_type -- use the 'requests' module or the 'selenium'. Will have different since the selenium webdriver does not have a similar response object when using the get method, and monitoring the behavior cannot be automated in the same way.\n",
    "    session -- requests.session object. For defining custom headers and proxies.\n",
    "    path2selenium -- str, sets the path to the geckodriver needed when using selenium.\n",
    "    n_tries -- int, defines the number of retries the *get* method will try to avoid random connection errors.\n",
    "    timeout -- int, seconds the get request will wait for the server to respond, again to avoid connection errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialization function defining parameters. \n",
    "    self.n_tries = n_tries # For avoiding triviel error e.g. connection errors, this defines how many times it will retry.\n",
    "    self.timeout = timeout # Defining the maximum time to wait for a server to response.\n",
    "    ## not implemented here, if you use selenium.\n",
    "    if connector_type=='selenium':\n",
    "      assert path2selenium!='', \"You need to specify the path to you geckodriver if you want to use Selenium\"\n",
    "      from selenium import webdriver \n",
    "      ## HIN download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "      assert os.path.isfile(path2selenium),'You need to insert a valid path2selenium the path to your geckodriver. You can download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases'\n",
    "      self.browser = webdriver.Firefox(executable_path=path2selenium) # start the browser with a path to the geckodriver.\n",
    "\n",
    "    self.connector_type = connector_type # set the connector_type\n",
    "    \n",
    "    if session: # set the custom session\n",
    "      self.session = session\n",
    "    else:\n",
    "      self.session = requests.session()\n",
    "    self.logfilename = logfile # set the logfile path\n",
    "    ## define header for the logfile\n",
    "    header = ['id','project','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "    if os.path.isfile(logfile):        \n",
    "      if overwrite_log==True:\n",
    "        self.log = open(logfile,'w')\n",
    "        self.log.write(';'.join(header))\n",
    "      else:\n",
    "        self.log = open(logfile,'a')\n",
    "    else:\n",
    "      self.log = open(logfile,'w')\n",
    "      self.log.write(';'.join(header))\n",
    "    ## load log \n",
    "    with open(logfile,'r') as f: # open file\n",
    "        \n",
    "      l = f.read().split('\\n') # read and split file by newlines.\n",
    "      ## set id\n",
    "      if len(l)<=1:\n",
    "        self.id = 0\n",
    "      else:\n",
    "        self.id = int(l[-1][0])+1\n",
    "            \n",
    "  def get(self,url,project_name):\n",
    "    \"\"\"Method for connector reliably to the internet, with multiple tries and simple error handling, as well as default logging function.\n",
    "    Input url and the project name for the log (i.e. is it part of mapping the domain, or is it the part of the final stage in the data collection).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- str, url\n",
    "    project_name -- str, Name used for analyzing the log. Use case could be the 'Mapping of domain','Meta_data_collection','main data collection'. \n",
    "    \"\"\"\n",
    "     \n",
    "    project_name = project_name.replace(';','-') # make sure the default csv seperator is not in the project_name.\n",
    "    if self.connector_type=='requests': # Determine connector method.\n",
    "      for _ in range(self.n_tries): # for loop defining number of retries with the requests method.\n",
    "        ratelimit()\n",
    "        t = time.time()\n",
    "        try: # error handling \n",
    "          response = self.session.get(url,timeout = self.timeout) # make get call\n",
    "\n",
    "          err = '' # define python error variable as empty assumming success.\n",
    "          success = True # define success variable\n",
    "          redirect_url = response.url # log current url, after potential redirects \n",
    "          dt = t - time.time() # define delta-time waiting for the server and downloading content.\n",
    "          size = len(response.text) # define variable for size of html content of the response.\n",
    "          response_code = response.status_code # log status code.\n",
    "          ## log...\n",
    "          call_id = self.id # get current unique identifier for the call\n",
    "          self.id+=1 # increment call id\n",
    "          #['id','project_name','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row to be written in the log.\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write log.\n",
    "          return response,call_id # return response and unique identifier.\n",
    "\n",
    "        except Exception as e: # define error condition\n",
    "          err = str(e) # python error\n",
    "          response_code = '' # blank response code \n",
    "          success = False # call success = False\n",
    "          size = 0 # content is empty.\n",
    "          redirect_url = '' # redirect url empty \n",
    "          dt = t - time.time() # define delta t\n",
    "\n",
    "          ## log...\n",
    "          call_id = self.id # define unique identifier\n",
    "          self.id+=1 # increment call_id\n",
    "\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write row to log.\n",
    "    else:\n",
    "      t = time.time()\n",
    "      ratelimit()\n",
    "      self.browser.get(url) # use selenium get method\n",
    "      ## log\n",
    "      call_id = self.id # define unique identifier for the call. \n",
    "      self.id+=1 # increment the call_id\n",
    "      err = '' # blank error message\n",
    "      success = '' # success blank\n",
    "      redirect_url = self.browser.current_url # redirect url.\n",
    "      dt = t - time.time() # get time for get method ... NOTE: not necessarily the complete load time.\n",
    "      size = len(self.browser.page_source) # get size of content ... NOTE: not necessarily correct, since selenium works in the background, and could still be loading.\n",
    "      response_code = '' # empty response code.\n",
    "      row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row \n",
    "      self.log.write('\\n'+';'.join(map(str,row))) # write row to log file.\n",
    "    # Using selenium it will not return a response object, instead you should call the browser object of the connector.\n",
    "    ## connector.browser.page_source will give you the html.\n",
    "      return call_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox(executable_path=r'/Users/marcusbjarupthogersen/Documents/geckodriver')\n",
    "from time import sleep\n",
    "connector = Connector('logfile_sds_trip_mother3.csv')\n",
    "\n",
    "# 27 pages of Danish written reviews. \n",
    "links_mother=[]\n",
    "for nummer in list(range(0, 10*27, 10)):\n",
    "    sleep(0.5)\n",
    "    url_nr = 'https://www.tripadvisor.com/Restaurant_Review-g189541-d1898372-Reviews-or{}-Mother-Copenhagen_Zealand.html'.format(nummer)\n",
    "    response, call_id = connector.get(url_nr,'scraping restaurants')\n",
    "    links_mother.append(url_nr)\n",
    "\n",
    "pd.read_csv('logfile_sds_trip_mother3.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox(executable_path=r'/Users/marcusbjarupthogersen/Documents/geckodriver')\n",
    "\n",
    "data = []\n",
    "for link in links_mother:\n",
    "    sleep(0.7)\n",
    "    N = []\n",
    "    driver.get(link)\n",
    "    driver.find_element_by_xpath(\"//span[contains(@class, 'ulBlueLinks')]\").click()\n",
    "    sleep(0.7)\n",
    "    for item in driver.find_elements_by_class_name('reviewSelector'):\n",
    "        review = item.find_element_by_class_name('partial_entry').text\n",
    "        N.append(review)\n",
    "    data.append(pd.DataFrame({'review': N}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_motherDK = pd.concat(data)\n",
    "trip_motherDK = trip_motherDK.to_csv(r'/Users/marcusbjarupthogersen/Documents/review_motherDK.csv')\n",
    "trip_motherDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox(executable_path=r'/Users/marcusbjarupthogersen/Documents/geckodriver')\n",
    "from time import sleep\n",
    "connector = Connector('logfile_sds_trip_fiskebar3.csv')\n",
    "\n",
    "# 16 pages of Danish written reviews. \n",
    "links_fiskebar=[]\n",
    "for nummer in list(range(0, 10*16, 10)):\n",
    "    sleep(0.5)\n",
    "    url_nr = 'https://www.tripadvisor.com/Restaurant_Review-g189541-d2085491-Reviews-or{}-Kodbyens_Fiskebar-Copenhagen_Zealand.html'.format(nummer)\n",
    "    response, call_id = connector.get(url_nr,'scraping restaurants')\n",
    "    links_fiskebar.append(url_nr)\n",
    "pd.read_csv('logfile_sds_trip_fiskebar3.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox(executable_path=r'/Users/marcusbjarupthogersen/Documents/geckodriver')\n",
    "\n",
    "data_fiskebar = []\n",
    "for link in links_fiskebar:\n",
    "    sleep(0.7)\n",
    "    N = []\n",
    "    driver.get(link)\n",
    "    driver.find_element_by_xpath(\"//span[contains(@class, 'ulBlueLinks')]\").click()\n",
    "    sleep(0.7)\n",
    "    for item in driver.find_elements_by_class_name('reviewSelector'):\n",
    "        review = item.find_element_by_class_name('partial_entry').text\n",
    "        N.append(review)\n",
    "    data_fiskebar.append(pd.DataFrame({'review': N}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_fiskebarDK = pd.concat(data_fiskebar)\n",
    "trip_fiskebarDK = trip_fiskebarDK.to_csv(r'/Users/marcusbjarupthogersen/Documents/review_fiskebarDK.csv')\n",
    "trip_fiskebarDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox(executable_path=r'/Users/marcusbjarupthogersen/Documents/geckodriver')\n",
    "from time import sleep\n",
    "connector = Connector('logfile_sds_trip_tight3.csv')\n",
    "\n",
    "# 20 pages of Danish written reviews. \n",
    "links_tight=[]\n",
    "for nummer in list(range(0, 10*20, 10)):\n",
    "    sleep(0.5)\n",
    "    url_nr = 'https://www.tripadvisor.dk/Restaurant_Review-g189541-d1528309-Reviews-or{}-Restaurant_Tight-Copenhagen_Zealand.html'.format(nummer)\n",
    "    response, call_id = connector.get(url_nr,'scraping restaurants')\n",
    "    links_tight.append(url_nr)\n",
    "pd.read_csv('logfile_sds_trip_tight3.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox(executable_path=r'/Users/marcusbjarupthogersen/Documents/geckodriver')\n",
    "\n",
    "data_tight = []\n",
    "for link in links_tight:\n",
    "    sleep(1)\n",
    "    N = []s\n",
    "    driver.get(link)\n",
    "    driver.find_element_by_xpath(\"//span[contains(@class, 'ulBlueLinks')]\").click()\n",
    "    sleep(1)\n",
    "    for item in driver.find_elements_by_class_name('reviewSelector'):\n",
    "        review = item.find_element_by_class_name('partial_entry').text\n",
    "        N.append(review)\n",
    "    data_tight.append(pd.DataFrame({'review': N}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_tightDK = pd.concat(data_tight)\n",
    "trip_tightDK = trip_tightDK.to_csv(r'/Users/marcusbjarupthogersen/Documents/review_tightDK.csv')\n",
    "trip_tightDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
